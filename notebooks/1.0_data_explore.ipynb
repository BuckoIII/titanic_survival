{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f3749401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843f896",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "\n",
    "1. transform data\n",
    "1. use a logisitc regression model as a a baseline for prediciton of the trainng set\n",
    "1. use a mulit-layed layered neural network to predict a binary prediction fo survival for he training set\n",
    "\n",
    "#### compeition docs \n",
    "https://www.kaggle.com/competitions/titanic/data\n",
    "\n",
    "#### data\n",
    "data dictionary\n",
    "        Variable\tDefinition\tKey\n",
    "        survival\t- Survival\t0 = No, 1 = Yes\n",
    "        pclass - \tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "        sex\t- Sex\t\n",
    "        Age\t- Age in years\t\n",
    "        sibsp\t- # of siblings / spouses aboard the Titanic\t\n",
    "        parch\t- # of parents / children aboard the Titanic\t\n",
    "        ticket\t- Ticket number\t\n",
    "        fare\t- Passenger fare\t\n",
    "        cabin\t- Cabin number\t\n",
    "        embarked\t- Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "#### activation funciton:\n",
    "- layer 1 - Relu\n",
    "- output layer - sigmoid\n",
    "\n",
    "#### formulas:\n",
    "- logistic regression:\n",
    "        sigmoid(w.T x  + b)\n",
    "- sigmoid:\n",
    "        np.sigmoid()\n",
    "- relu:\n",
    "        np.maximum(arr, 0)\n",
    "- back propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fd26c7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  \\\n",
       "PassengerId                     \n",
       "1                   0       3   \n",
       "2                   1       1   \n",
       "3                   1       3   \n",
       "4                   1       1   \n",
       "5                   0       3   \n",
       "...               ...     ...   \n",
       "887                 0       2   \n",
       "888                 1       1   \n",
       "889                 0       3   \n",
       "890                 1       1   \n",
       "891                 0       3   \n",
       "\n",
       "                                                          Name     Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
       "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
       "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "5                                     Allen, Mr. William Henry    male  35.0   \n",
       "...                                                        ...     ...   ...   \n",
       "887                                      Montvila, Rev. Juozas    male  27.0   \n",
       "888                               Graham, Miss. Margaret Edith  female  19.0   \n",
       "889                   Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN   \n",
       "890                                      Behr, Mr. Karl Howell    male  26.0   \n",
       "891                                        Dooley, Mr. Patrick    male  32.0   \n",
       "\n",
       "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                          \n",
       "1                1      0         A/5 21171   7.2500   NaN        S  \n",
       "2                1      0          PC 17599  71.2833   C85        C  \n",
       "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "4                1      0            113803  53.1000  C123        S  \n",
       "5                0      0            373450   8.0500   NaN        S  \n",
       "...            ...    ...               ...      ...   ...      ...  \n",
       "887              0      0            211536  13.0000   NaN        S  \n",
       "888              0      0            112053  30.0000   B42        S  \n",
       "889              1      2        W./C. 6607  23.4500   NaN        S  \n",
       "890              0      0            111369  30.0000  C148        C  \n",
       "891              0      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "\n",
    "DATA_PATH = Path('/Users/tommadden/titanic_survival/data')\n",
    "p = Path(DATA_PATH, 'titanic', 'train.csv')\n",
    "train_df = pd.read_csv(p, index_col=0)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d2e26e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "892                 0\n",
       "893                 1\n",
       "894                 0\n",
       "895                 0\n",
       "896                 1\n",
       "...               ...\n",
       "1305                0\n",
       "1306                1\n",
       "1307                0\n",
       "1308                0\n",
       "1309                0\n",
       "\n",
       "[418 rows x 1 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load testing data\n",
    "\n",
    "p = Path(DATA_PATH, 'titanic', 'test.csv')\n",
    "test_df = pd.read_csv(p, index_col=0)\n",
    "test_df\n",
    "\n",
    "p = Path(DATA_PATH, 'titanic', 'gender_submission.csv')\n",
    "test_Y = pd.read_csv(p, index_col=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "70402ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "\n",
    "    # transform input parameters\n",
    "    input_df = df\n",
    "    \n",
    "    # sex classification: male = 1, female = 0\n",
    "    input_df['Sex'] = df.Sex.eq('male').mul(1)\n",
    "\n",
    "    # fill NaN age to average age (29.5)\n",
    "    input_df['Age'] = input_df['Age'].fillna(29.5)\n",
    "\n",
    "    input_df['Cabin']\n",
    "    input_df.Cabin.fillna(0, inplace = True)\n",
    "    # set records with multiple cabin assignments to first cabin assingment (assumes this will not introduce bias)\n",
    "    input_df['CabinLevel'] = [i[0] if type(i) == list else i for i in input_df.Cabin.str.split().values]\n",
    "    # regex to isolate first letter of cabin name (presumed to be the level of the ship)\n",
    "    input_df['CabinLevel'] = [i[0] if type(i) == list else i for i in input_df.Cabin.str.findall(r'[A-Z]').values]\n",
    "\n",
    "    # cabin counts to columns\n",
    "    input_df['CabinLevel'].fillna(0, inplace = True)\n",
    "    levels = input_df['CabinLevel'].unique()\n",
    "    for i in levels:\n",
    "        input_df[str('CabinLevel'+str(i))] = np.where(input_df['CabinLevel'] == i, 1, 0)\n",
    "\n",
    "    # parse and store cabin number from cabin cloumn into canibno cloumn\n",
    "    input_df['CabinNo'] = [i[0] if type(i) == list else i for i in input_df.Cabin.str.split().values]\n",
    "\n",
    "    num_arr = input_df.CabinNo.str.findall(r'[0-9]+').values\n",
    "\n",
    "    CabinNoArr = []\n",
    "    for i in num_arr:\n",
    "        if type(i) == list:\n",
    "            try:\n",
    "              CabinNoArr.append(i[0])  \n",
    "            except:\n",
    "                CabinNoArr.append(i)\n",
    "        else:\n",
    "            CabinNoArr.append(i)\n",
    "\n",
    "    input_df['CabinNo'] = CabinNoArr    \n",
    "    input_df['CabinNo'].fillna(0, inplace=True)\n",
    "\n",
    "    # include only unwanted comlumns\n",
    "    filt = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch',\n",
    "           'Fare', 'CabinLevel0', 'CabinLevelC',\n",
    "           'CabinLevelE', 'CabinLevelG', 'CabinLevelD', 'CabinLevelA',\n",
    "           'CabinLevelB', 'CabinLevelF']\n",
    "    input_df = input_df[filt]\n",
    "\n",
    "    # exclude unwanted columns\n",
    "#     input_df = input_df.loc[:, ~input_df.columns.isin(['Survived','Name','Ticket', 'Embarked','Cabin','CabinLevel'])]\n",
    "\n",
    "    return input_df\n",
    "\n",
    "train_df = clean_data(train_df)\n",
    "train_Y = df.Survived\n",
    "test_df = clean_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "567451a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'CabinLevel0',\n",
       "        'CabinLevelC', 'CabinLevelE', 'CabinLevelG', 'CabinLevelD',\n",
       "        'CabinLevelA', 'CabinLevelB', 'CabinLevelF'],\n",
       "       dtype='object'),\n",
       " Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'CabinLevel0',\n",
       "        'CabinLevelC', 'CabinLevelE', 'CabinLevelG', 'CabinLevelD',\n",
       "        'CabinLevelA', 'CabinLevelB', 'CabinLevelF'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns, train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1c470244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_matrix(matrix, labels):\n",
    "    \n",
    "    # transform df variables to matrix\n",
    "\n",
    "    m = matrix.shape[0]\n",
    "    n = matrix.shape[1]\n",
    "\n",
    "    # pandas df to matrix & transpose so that there are 16 features and 891 training examples\n",
    "    # each column is a single training example, each column is a feature\n",
    "    matrix = matrix.values.T\n",
    "    labels = labels.values.reshape(1,m)\n",
    "\n",
    "    assert matrix.shape == (n,m) \n",
    "    assert labels.shape == (1,m)\n",
    "\n",
    "    print(matrix.shape, labels.shape)\n",
    "    \n",
    "    return matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36b0eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 891) (1, 891)\n",
      "(14, 418) (1, 418)\n"
     ]
    }
   ],
   "source": [
    "# transform variables to matrix\n",
    "X_train, Y_train = df_to_matrix(train_df, train_Y)\n",
    "\n",
    "X_test, Y_test = df_to_matrix(test_df, test_Y )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b0324683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notes\n",
    "arr = np.array([i * 0.01 for i in range(-10,10)])\n",
    "# tanh fucntion\n",
    "np.tanh(arr)\n",
    "\n",
    "# relu function\n",
    "np.maximum(arr,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "313df9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning specialisatoin logistic regression helper functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1 / (1+np.exp(1)**-z)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias) of type float\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros(dim).reshape(dim,1)\n",
    "    b = 0.0\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    # forward propagation\n",
    "    A = sigmoid(np.dot(w.T.astype(float), X.astype(float)) + b)\n",
    "    cost = - np.mean(Y * np.log(A) + (1 - Y ) * np.log(1 - A))\n",
    "    \n",
    "    # back propagation\n",
    "    dw = 1/m * np.dot(X, (A-Y).T)\n",
    "    db = np.mean(A-Y)\n",
    "        \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "                \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of yhat = y\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b) \n",
    "        \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0,i] >= 0.5 :\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "                    \n",
    "    return Y_prediction\n",
    "\n",
    "\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to True to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    # initialize parameters with zeros \n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # calculates loss vs training set, calculates derivates and updates w & b\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # updates w,b to optisied values\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    " \n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "610a84ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: nan\n",
      "Cost after iteration 200: nan\n",
      "Cost after iteration 300: nan\n",
      "Cost after iteration 400: nan\n",
      "Cost after iteration 500: nan\n",
      "Cost after iteration 600: nan\n",
      "Cost after iteration 700: nan\n",
      "Cost after iteration 800: nan\n",
      "Cost after iteration 900: nan\n",
      "Cost after iteration 1000: nan\n",
      "Cost after iteration 1100: nan\n",
      "Cost after iteration 1200: nan\n",
      "Cost after iteration 1300: nan\n",
      "Cost after iteration 1400: nan\n",
      "Cost after iteration 1500: nan\n",
      "Cost after iteration 1600: nan\n",
      "Cost after iteration 1700: nan\n",
      "Cost after iteration 1800: nan\n",
      "Cost after iteration 1900: nan\n",
      "train accuracy: 69.36026936026936 %\n",
      "test accuracy: 74.16267942583733 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/mm75c5z52274c3ylzwr2jg200000gp/T/ipykernel_1062/4240751090.py:60: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = - np.mean(Y * np.log(A) + (1 - Y ) * np.log(1 - A))\n",
      "/var/folders/xg/mm75c5z52274c3ylzwr2jg200000gp/T/ipykernel_1062/4240751090.py:60: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = - np.mean(Y * np.log(A) + (1 - Y ) * np.log(1 - A))\n",
      "/var/folders/xg/mm75c5z52274c3ylzwr2jg200000gp/T/ipykernel_1062/4240751090.py:13: RuntimeWarning: overflow encountered in power\n",
      "  s = 1 / (1+np.exp(1)**-z)\n"
     ]
    }
   ],
   "source": [
    "# implement model\n",
    "\n",
    "log_model = model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "442acaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'costs': [array(0.69314718),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan),\n",
       "  array(nan)],\n",
       " 'Y_prediction_test': array([[0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "         1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "         0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "         1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "         0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "         0., 1.]]),\n",
       " 'Y_prediction_train': array([[0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "         0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "         0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "         1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "         1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "         0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "         1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "         1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "         1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
       "         0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "         1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "         1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "         1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.]]),\n",
       " 'w': array([[  3.67042276],\n",
       "        [-71.01028017],\n",
       "        [ -2.0157911 ],\n",
       "        [-38.55987462],\n",
       "        [ -9.85637803],\n",
       "        [  8.02901056],\n",
       "        [ -8.09361349],\n",
       "        [ -2.47619786],\n",
       "        [  8.17321169],\n",
       "        [  0.07561265],\n",
       "        [  7.62574608],\n",
       "        [ -0.42775479],\n",
       "        [  5.58624498],\n",
       "        [  3.00543901]]),\n",
       " 'b': 12.917515378707744,\n",
       " 'learning_rate': 0.5,\n",
       " 'num_iterations': 2000}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2999dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.    ,  3.    ,  2.    ,  3.    ,  3.    ,  3.    ],\n",
       "       [ 1.    ,  0.    ,  1.    ,  1.    ,  0.    ,  1.    ],\n",
       "       [34.5   , 47.    , 62.    , 27.    , 22.    , 14.    ],\n",
       "       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  1.    ,  0.    ],\n",
       "       [ 7.8292,  7.    ,  9.6875,  8.6625, 12.2875,  9.225 ],\n",
       "       [ 1.    ,  1.    ,  1.    ,  1.    ,  1.    ,  1.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_exp = X_test.T[0:6].T\n",
    "X_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d137f268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_exp = Y_test.T[0:6]\n",
    "Y_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "90cc2dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = log_model['w']\n",
    "b = log_model['b']\n",
    "\n",
    "# X_exp, Y_exp, w, b\n",
    "\n",
    "predict(w,b, X_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9ba50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
